{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1058293c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist([1,5,5,7,7,9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(\"This is a pretty cool tool!\")\n",
    "word_length = [len(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 2, 1, 6, 4, 4, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a09b7e748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(word_length)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the script into lines: lines\n",
    "lines = holy_grail.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word counts with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The cat is in the box. The cat likes the box. The box is over the cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'.': 2,\n",
       "         'The': 3,\n",
       "         'box': 3,\n",
       "         'cat': 3,\n",
       "         'in': 1,\n",
       "         'is': 2,\n",
       "         'likes': 1,\n",
       "         'over': 1,\n",
       "         'the': 3})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3), ('cat', 3), ('the', 3)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple text pre processing\n",
    "\n",
    "create BOW\n",
    "Lowercasing\n",
    "shorten words to their room - Lemmatization/Stemming\n",
    "Remove stopwords, punctuation or unwanted tokens like rarely appearing words\n",
    "experiement with combination of these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'cat',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " 'box',\n",
       " 'the',\n",
       " 'cat',\n",
       " 'likes',\n",
       " 'the',\n",
       " 'box',\n",
       " 'the',\n",
       " 'box',\n",
       " 'is',\n",
       " 'over',\n",
       " 'the',\n",
       " 'cat']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stops = [t for t in tokens if t not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'box', 'cat', 'likes', 'box', 'box', 'cat']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "my_docs = ['The movie was about a speceship and aliens.',\n",
    "           'I really liked the movie.',\n",
    "           'Awesome action scenes, but boring characters.',\n",
    "           'The movie was aweful! I hate the alien films.',\n",
    "           'Space is cool! I liked the movie.',\n",
    "           'More space films, please.']\n",
    "\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'movie', 'was', 'about', 'a', 'speceship', 'and', 'aliens', '.'],\n",
       " ['i', 'really', 'liked', 'the', 'movie', '.'],\n",
       " ['awesome', 'action', 'scenes', ',', 'but', 'boring', 'characters', '.'],\n",
       " ['the',\n",
       "  'movie',\n",
       "  'was',\n",
       "  'aweful',\n",
       "  '!',\n",
       "  'i',\n",
       "  'hate',\n",
       "  'the',\n",
       "  'alien',\n",
       "  'films',\n",
       "  '.'],\n",
       " ['space', 'is', 'cool', '!', 'i', 'liked', 'the', 'movie', '.'],\n",
       " ['more', 'space', 'films', ',', 'please', '.']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x1a09c8fac8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 19,\n",
       " ',': 12,\n",
       " '.': 0,\n",
       " 'a': 1,\n",
       " 'about': 2,\n",
       " 'action': 13,\n",
       " 'alien': 20,\n",
       " 'aliens': 3,\n",
       " 'and': 4,\n",
       " 'aweful': 21,\n",
       " 'awesome': 14,\n",
       " 'boring': 15,\n",
       " 'but': 16,\n",
       " 'characters': 17,\n",
       " 'cool': 24,\n",
       " 'films': 22,\n",
       " 'hate': 23,\n",
       " 'i': 9,\n",
       " 'is': 25,\n",
       " 'liked': 10,\n",
       " 'more': 27,\n",
       " 'movie': 5,\n",
       " 'please': 28,\n",
       " 'really': 11,\n",
       " 'scenes': 18,\n",
       " 'space': 26,\n",
       " 'speceship': 6,\n",
       " 'the': 7,\n",
       " 'was': 8}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)], [(0, 1), (5, 1), (7, 1), (9, 1), (10, 1), (11, 1)], [(0, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)], [(0, 1), (5, 1), (7, 2), (8, 1), (9, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)], [(0, 1), (5, 1), (7, 1), (9, 1), (10, 1), (19, 1), (24, 1), (25, 1), (26, 1)], [(0, 1), (12, 1), (22, 1), (26, 1), (27, 1), (28, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(word_id, word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency - inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "tfidf = TfidfModel(corpus)\n",
    "tfidf(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfModel\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Named entity recognition (NER)\n",
    "\n",
    "tagged_sent = nltk.pos_tage(tokenized_sentences)\n",
    "print(nltk.ne_chunk(tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the article into sentences: sentences\n",
    "sentences = nltk.sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(l) for l in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "nlp.entity\n",
    "\n",
    "doc = nlp(\"\"\"Berlin is the capital of Germany; and the residence of Chancellor of Angela Merkel\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Berlin, Germany, Angela Merkel)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berlin GPE\n"
     ]
    }
   ],
   "source": [
    "print(doc.ents[0], doc.ents[0].label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "nlp = spacy.load('en', tagger=False, parser=False, matcher=False)\n",
    "\n",
    "# Create a new document: doc\n",
    "doc = nlp(article)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual NER with polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.text import Text\n",
    "ptext = Text(text)\n",
    "\n",
    "ptext.entities\n",
    "\n",
    "# Create a new text object using Polyglot's Text class: txt\n",
    "txt = Text(article)\n",
    "\n",
    "# Print each of the entities found\n",
    "for ent in txt.entities:\n",
    "    print(ent)\n",
    "    \n",
    "# Print the type of each entity\n",
    "print(type(ent))\n",
    "\n",
    "# Create the list of tuples: entities\n",
    "entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
    "# Print the entities\n",
    "print(entities)\n",
    "\n",
    "\n",
    "# Initialize the count variable: count\n",
    "count = 0\n",
    "\n",
    "# Iterate over all the entities\n",
    "for ent in txt.entities:\n",
    "    # Check whether the entity contains 'Márquez' or 'Gabo'\n",
    "    if (\"Márquez\" in ent) or (\"Gabo\" in ent):\n",
    "        # Increment count\n",
    "        count += 1\n",
    "\n",
    "# Print count\n",
    "print(count)\n",
    "\n",
    "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
    "percentage = count * 1.0 / len(txt.entities)\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of spaCy entities keeping only their text: spacy_ents\n",
    "spacy_ents = {e.text for e in doc.ents} \n",
    "\n",
    "# Create a set of the intersection between the spacy and polyglot entities: ensemble_ents\n",
    "ensemble_ents = spacy_ents.intersection(poly_ents)\n",
    "\n",
    "# Print the common entities\n",
    "print(ensemble_ents)\n",
    "\n",
    "# Calculate the number of entities not included in the new ensemble set of entities: num_left_out\n",
    "num_left_out = len(spacy_ents.union(poly_ents)) - len(ensemble_ents)\n",
    "print(num_left_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised ML using NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df.label\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(tfidf_df.columns) - set(count_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,1,0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "\n",
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
