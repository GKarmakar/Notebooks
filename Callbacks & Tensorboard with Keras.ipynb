{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import Callback, TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to use the MNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/tmp/data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST contains 28x28 images: 55,000 training images and 10,000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOGDIR = './graphs'\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 10\n",
    "\n",
    "# Layers\n",
    "HL_1 = 1000\n",
    "HL_2 = 500\n",
    "\n",
    "# Other Parameters\n",
    "INPUT_SIZE = 28*28\n",
    "N_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct the network specifying detailed implementation of each layer. The deep neural network we use has:\n",
    "- Input _layer_ with inputs equal to the pixel intensities of every image. \n",
    "- First Hidden Layer with 1000 neurons.\n",
    "- Second Hidden Layer with 500 neurons.\n",
    "- Output Layer with 10 neurons to represent 10 classes of images corresponding to digits 0-9.\n",
    "\n",
    "The following is a tensorflow implementation of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "\timages = tf.placeholder(tf.float32, [None, INPUT_SIZE] , name=\"images\")\n",
    "\tlabels = tf.placeholder(tf.float32, [None, N_CLASSES], name=\"labels\")\n",
    "\n",
    "def fc_layer(x, layer, size_out, activation=None):\n",
    "\twith tf.name_scope(layer):\n",
    "\t\tsize_in = int(x.shape[1])\n",
    "\t\tW = tf.Variable(tf.random_normal([size_in, size_out]) , name=\"weights\") \n",
    "\t\tb = tf.Variable(tf.constant(-1, dtype=tf.float32, shape=[size_out]), name=\"biases\")\n",
    "\n",
    "\t\twx_plus_b = tf.add(tf.matmul(x, W), b)\n",
    "\t\tif activation: \n",
    "\t\t\treturn activation(wx_plus_b)\n",
    "\t\treturn wx_plus_b\n",
    "\n",
    "fc_1 = fc_layer(images, \"fc_1\",  HL_1, tf.nn.relu)\n",
    "fc_2 = fc_layer(fc_1, \"fc_2\", HL_2, tf.nn.relu)\n",
    "dropped = tf.nn.dropout(fc_2, keep_prob=0.9)\n",
    "y = fc_layer(dropped, \"output\", N_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks terse enough already, but we can go much smaller using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000, input_dim=INPUT_SIZE, activation=\"relu\"))\n",
    "model.add(Dense(500, activation=\"relu\"))\n",
    "model.add(Dropout(rate=0.9))\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is self explanatory, but let us gloss over it anyways. \n",
    "- In the first line, we initialize a _Sequential_ model. Such a model is a linear stack of layers. Deep and Convolutional Neural Networks follow this architecture. We will construct a deep neural network to model this MNIST dataset.\n",
    "- On the next line, a new layer is added to the empty model using the _add_ method. We are adding a fully connected hidden layer with 1000 neurons. Each neuron uses the relu activation. \n",
    "- On the 3rd line, we create another fully connected hidden layer with 500 neurons and apply a relu activation.\n",
    "- Between two consecutive layers of a deep neural network, every neuron is connected to every other. So every input sample passes thorough every neuron and is learned by every neuron. Since neurons learn similar information, there is a high chance of corelation between them. This in turn means the information acrued by individual neurons becomes less significant, leadning to overfitting. Dropout is a method of regularization where we randomly turn off neurons and force the network to learn along different neuron paths. This enhances generalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For forward layers, activations can either be used through an Activation layer or through the activation argument. So we can write the code block above in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000, input_dim=INPUT_SIZE))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(500))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(rate=0.9))\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the first method because it is more intuitive for deep neural networks. The activation is a part of the same neuron and does not constitute a layer on it's own. This second notation better suits convolutional neural nets with separate convolution layers, activation layers and pooling layers.\n",
    "\n",
    "We now construct the remaining parts of the computation graph by defining loss, the optimizer, and evaluation metric which is simple accuracy in this case. The following tensorflow code accomplishes this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "\tloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=labels))\n",
    "\ttf.summary.scalar('loss', loss)\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "\ttrain = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "with tf.name_scope('evaluation'):\n",
    "\tcorrect = tf.equal( tf.argmax(y, 1), tf.argmax(labels, 1) )\n",
    "\taccuracy = tf.reduce_mean( tf.cast(correct, dtype=tf.float32) )\n",
    "\ttf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently in Keras,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "\toptimizer=\"Adam\",\n",
    "\tloss=\"categorical_crossentropy\",\n",
    "\tmetrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a scalar graph on a Tensorboard to visualize the change in accuracy and loss over time. \n",
    "\n",
    "We are training the samples in batches of 1000 over 10 epochs. In other words, all 55,000 samples are partitioned into 55 _batches_ of 1000 samples. Iterating over all batches once constituted 1 epoch. This process is repested 10 times to complete the training.\n",
    "\n",
    "Our task is log the values of accuracy and loss after every batch or epoch. This is done using _callbacks_. A **Callback** is a function that is triggered by an event. E.g. Some web developers may know this as the javascript handlers executed after an AJAX call is made. \n",
    "\n",
    "In our case, we need to log the batch(epoch) accuracy after evey batch(epoch) is processed. Keras has built-in callbacks that extend `keras.callbacks.Callback` with the following class definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Callback(object):\n",
    "    \"\"\"Abstract base class used to build new callbacks.\n",
    "\n",
    "    # Properties\n",
    "        params: dict. Training parameters\n",
    "            (eg. verbosity, batch size, number of epochs...).\n",
    "        model: instance of `keras.models.Model`.\n",
    "            Reference of the model being trained.\n",
    "\n",
    "    The `logs` dictionary that callback methods\n",
    "    take as argument will contain keys for quantities relevant to\n",
    "    the current batch or epoch.\n",
    "\n",
    "    Currently, the `.fit()` method of the `Sequential` model class\n",
    "    will include the following quantities in the `logs` that\n",
    "    it passes to its callbacks:\n",
    "\n",
    "        on_epoch_end: logs include `acc` and `loss`, and\n",
    "            optionally include `val_loss`\n",
    "            (if validation is enabled in `fit`), and `val_acc`\n",
    "            (if validation and accuracy monitoring are enabled).\n",
    "        on_batch_begin: logs include `size`,\n",
    "            the number of samples in the current batch.\n",
    "        on_batch_end: logs include `loss`, and optionally `acc`\n",
    "            (if accuracy monitoring is enabled).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.validation_data = None\n",
    "\n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could extend this callback and implement override the `on_batch_end` (`on_epoch_end`) method, but keras already has a `TensorBoard` callback that extends this class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch-wise plots\n",
    "\n",
    "`keras.Callbacks.TensorBoard` implements the `on_epoch_end` method and logs the accuracy and loss using the `FileWriter`. The following is a snippet of the code exectuted behind the scenes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def on_epoch_end(self, epoch, logs=None):\n",
    "    logs = logs or {}\n",
    "\n",
    "    if not self.validation_data and self.histogram_freq:\n",
    "        raise ValueError('If printing histograms, validation_data must be '\n",
    "                         'provided, and cannot be a generator.')\n",
    "    if self.validation_data and self.histogram_freq:\n",
    "        if epoch % self.histogram_freq == 0:\n",
    "\n",
    "            val_data = self.validation_data\n",
    "            tensors = (self.model.inputs +\n",
    "                       self.model.targets +\n",
    "                       self.model.sample_weights)\n",
    "\n",
    "            if self.model.uses_learning_phase:\n",
    "                tensors += [K.learning_phase()]\n",
    "\n",
    "            assert len(val_data) == len(tensors)\n",
    "            val_size = val_data[0].shape[0]\n",
    "            i = 0\n",
    "            while i < val_size:\n",
    "                step = min(self.batch_size, val_size - i)\n",
    "                if self.model.uses_learning_phase:\n",
    "                    # do not slice the learning phase\n",
    "                    batch_val = [x[i:i + step] for x in val_data[:-1]]\n",
    "                    batch_val.append(val_data[-1])\n",
    "                else:\n",
    "                    batch_val = [x[i:i + step] for x in val_data]\n",
    "                assert len(batch_val) == len(tensors)\n",
    "                feed_dict = dict(zip(tensors, batch_val))\n",
    "                result = self.sess.run([self.merged], feed_dict=feed_dict)\n",
    "                summary_str = result[0]\n",
    "                self.writer.add_summary(summary_str, epoch)\n",
    "                i += self.batch_size\n",
    "\n",
    "    if self.embeddings_freq and self.embeddings_ckpt_path:\n",
    "        if epoch % self.embeddings_freq == 0:\n",
    "            self.saver.save(self.sess,\n",
    "                            self.embeddings_ckpt_path,\n",
    "                            epoch)\n",
    "\n",
    "    for name, value in logs.items():\n",
    "        if name in ['batch', 'size']:\n",
    "            continue\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = value.item()\n",
    "        summary_value.tag = name\n",
    "        self.writer.add_summary(summary, epoch)\n",
    "    self.writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the 55 batch samples have been processed, 1 epoch is complete. Since 1 epoch is processed, the function above is executed by passing 2 parameters:\n",
    "- **epoch**: The epoch number just executed\n",
    "- **logs**: Dictionary of logged values with the field as the _key_ and corresponging value as the _value_ for each entry. The log has 4 entries:\n",
    "    - **_epoch_**: Epoch number executed. It ranges from 0 to 9 in our case.\n",
    "    - **_size_**: Number of entries in each epoch. It is 55,000 for all epochs.\n",
    "    - **_loss_**: The loss value computed after processing the epoch\n",
    "    - **_acc_**: Accuracy performance metric after processing the epoch\n",
    "    \n",
    "Here is a sample `logs`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dict_items([('epoch', 1), ('size', 55000), ('loss', 1.0543008), ('acc', 0.90300001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While using `keras.callbacks.TensorBoard` as a callback passing the default optional parameter values, only the last 10 lines of the `on_epoch_end` function are executed i.e. the last for loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, value in logs.items():\n",
    "    if name in ['batch', 'size']:\n",
    "        continue\n",
    "    summary = tf.Summary()\n",
    "    summary_value = summary.value.add()\n",
    "    summary_value.simple_value = value.item()\n",
    "    summary_value.tag = name\n",
    "    self.writer.add_summary(summary, epoch)\n",
    "self.writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code creates a `tensorflow.Summary` object to record _accuracy_ and _loss_ for every epoch. Since this is done by the `keras.callbacks.TensorBoard`, this function should be executed after every epoch is processed. (Hence, it's a callback.). This callback is specified during model training using the _fit_ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 11s 205us/step - loss: 0.7956 - acc: 0.7478\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 11s 208us/step - loss: 0.3206 - acc: 0.9113\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 11s 207us/step - loss: 0.2297 - acc: 0.9373\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 11s 205us/step - loss: 0.1759 - acc: 0.9523\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 11s 207us/step - loss: 0.1435 - acc: 0.9606\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 11s 207us/step - loss: 0.1156 - acc: 0.9685\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 12s 210us/step - loss: 0.0986 - acc: 0.9735\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 13s 228us/step - loss: 0.0835 - acc: 0.9768\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 12s 216us/step - loss: 0.0730 - acc: 0.9792\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 11s 208us/step - loss: 0.0612 - acc: 0.9827\n"
     ]
    }
   ],
   "source": [
    "cb = TensorBoard()\n",
    "\n",
    "history_callback = model.fit(\n",
    "\tx=mnist.train.images, \n",
    "\ty=mnist.train.labels, \n",
    "\tepochs=EPOCHS, \n",
    "\tbatch_size=BATCH_SIZE,\n",
    "\tcallbacks=[cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _fit_ method returns a `keras.callbacks.History` object that records accuracy and loss after successive epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': [0, 1],\n",
       " 'history': {'acc': [0.93967272910204802, 0.95405454418875957],\n",
       "  'loss': [0.2160060319033536, 0.16850824870846487]},\n",
       " 'model': <keras.models.Sequential at 0x12ac58978>,\n",
       " 'params': {'batch_size': 1000,\n",
       "  'do_validation': False,\n",
       "  'epochs': 2,\n",
       "  'metrics': ['loss', 'acc'],\n",
       "  'samples': 55000,\n",
       "  'steps': None,\n",
       "  'verbose': 1},\n",
       " 'validation_data': []}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_callback.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the performance of the model on the test set using `keras.models.Sequential.evaluate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 188us/step\n",
      "score =  [0.10170452898396179, 0.96860000000000002]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(\n",
    "\tx=mnist.test.images,\n",
    "\ty=mnist.test.labels)\n",
    "\n",
    "print(\"score = \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first parameter is the loss on the test set and the second is accuracy. The following image is the tensorboard plots of the same Vs _number of epochs_.\n",
    "\n",
    "<img src=\"../mics/epoch_wise.png\" alt=\"tensorboard of 10 epochs\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch-wise plots\n",
    "\n",
    "We were able to use the built in `TensorBoard` callback to plot accuracy and loss for every epoch. However, what if we want the status for every _batch_ processed? The implementation of `keras.callbacks.TensorBoard` doesn't include `on_batch_end` or `on_bach_begin` methods. Thus, we extent this class and implement these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Batched_TensorBoard(TensorBoard):\n",
    "\n",
    "    def __init__(self):\n",
    "\t    self.log_dir = \"./log_dir\"\n",
    "\t    self.batch_writer = tf.summary.FileWriter(self.log_dir) # Created here as site-packages/keras/callback.py\n",
    "\t    self.step = 0 # Initialization\n",
    "\t    super().__init__(self.log_dir) # Execute TensorBoard's constructor, passing the log directory\n",
    "\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        \"\"\"Called after every batch\"\"\"\n",
    "\n",
    "        for name, value in logs.items():\n",
    "            if name in ['acc', 'loss']:\n",
    "                summary = tf.Summary()\n",
    "                summary_value = summary.value.add() #Empty\n",
    "                summary_value.simple_value = value.item() # 0.87 (Accuracy Value)\n",
    "                summary_value.tag = name #if \"acc\", tag = \"accuracy\" for more defined tags on the tensorboard\n",
    "                self.batch_writer.add_summary(summary, self.step) \n",
    "    \n",
    "        self.batch_writer.flush()\n",
    "        self.step += 1 # Iterated over every batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a FileWriter called `batch_writer` and not just `writer` because I didn't want this plot to interfere with the epoch-wise plot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use `Batched_TensorBoard` instance as our callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 11s 208us/step - loss: 0.7889 - acc: 0.7522\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 14s 247us/step - loss: 0.3146 - acc: 0.9115\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 13s 229us/step - loss: 0.2199 - acc: 0.9400\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 12s 225us/step - loss: 0.1714 - acc: 0.9532\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 12s 216us/step - loss: 0.1396 - acc: 0.96271s - loss: 0.1400 - acc: \n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 11s 209us/step - loss: 0.1145 - acc: 0.9681\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 13s 242us/step - loss: 0.0947 - acc: 0.9737\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 12s 211us/step - loss: 0.0831 - acc: 0.9776\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 11s 202us/step - loss: 0.0696 - acc: 0.9807\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 11s 209us/step - loss: 0.0610 - acc: 0.9834\n"
     ]
    }
   ],
   "source": [
    "cb = Batched_TensorBoard()\n",
    "\n",
    "history_callback = model.fit(\n",
    "\tx=mnist.train.images, \n",
    "\ty=mnist.train.labels, \n",
    "\tepochs=EPOCHS, \n",
    "\tbatch_size=BATCH_SIZE,\n",
    "\tcallbacks=[cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history_callback once again returns the accuracy and losses after every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': [0, 1],\n",
       " 'history': {'acc': [0.96309090635993266, 0.96879999962720009],\n",
       "  'loss': [0.13495511046864769, 0.11196692531759089]},\n",
       " 'model': <keras.models.Sequential at 0x12ac58978>,\n",
       " 'params': {'batch_size': 1000,\n",
       "  'do_validation': False,\n",
       "  'epochs': 2,\n",
       "  'metrics': ['loss', 'acc'],\n",
       "  'samples': 55000,\n",
       "  'steps': None,\n",
       "  'verbose': 1},\n",
       " 'validation_data': []}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_callback.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more detailed batchwise status, the tensorboard now includes plots for every batch processed.\n",
    "\n",
    "<img src=\"../mics/batch_wise.png\" alt=\"tensorboard of 10 batchs\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
