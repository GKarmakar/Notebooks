{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load nltk module\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download corpus\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import reuters corpus data\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check what is there in the corpus \n",
    "files = reuters.fileids()\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can check the contents of any of the file\n",
    "words16097 = reuters.words(['test/16097'])\n",
    "print(words16097)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run again to see the next entry\n",
    "words20 = reuters.words(['test/16097'])[:20]\n",
    "print(words20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reutersGeneres = reuters.categories()\n",
    "print(reutersGeneres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print words from two categories as separate sentence\n",
    "for w in reuters.words(categories=['bop','cocoa']):\n",
    "  print(w+' ',end='')\n",
    "  if(w is '.'):\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import CategorizedPlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First and foremost, you will need to download the dataset from the Internet. Here's the link: http://www.cs.cornell.edu/people/pabo/movie-review-data/mix20_rand700_tokens_cleaned.zip). Download the dataset, unzip it, and store the resultant Reviews directory at a secure location on your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the corpus that we have downloaded is already categorized, we will use CategorizedPlaintextCorpusReader to read and load the given corpus. This way, we can be sure that the categories of the corpus are captured, in this case, positive and negative.\n",
    "\n",
    "Now we will read the corpus. We need to know the absolute path of the Reviews folder that we unzipped from the downloaded file from Cornell. Add the following four lines of code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quintessential of this recipe is the CategorizedPlaintextCorpusReader class of NLTK. Since we already know that the corpus we have downloaded is categorized, we only need provide appropriate arguments when creating the reader object. The implementation of the CategorizedPlaintextCorpusReader class internally takes care of loading the samples in appropriate buckets ('pos' and 'neg' in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = CategorizedPlaintextCorpusReader(r'./data/Reviews/tokens', r'.*\\.txt', cat_pattern=r'(\\w+)/*')\n",
    "print(reader.categories())\n",
    "print(reader.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review sample revoews from positive and negative folders\n",
    "posFiles = reader.fileids(categories='pos')\n",
    "negFiles = reader.fileids(categories='neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reader.fileids() method takes the argument category name. As you can see, what we are trying to do in the preceding two lines of code is straightforward and intuitive.\n",
    "Now let's select a file randomly from each of the lists of posFiles and negFiles. To do so, we will need the randint() function from the random library of Python. Add the following lines of code and we shall elaborate what exactly we did immediately after:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select random files from positive and negative folder and print the contents\n",
    "from random import randint\n",
    "fileP = posFiles[randint(0,len(posFiles)-1)]\n",
    "fileN = negFiles[randint(0, len(posFiles) - 1)]\n",
    "print(fileP)\n",
    "print(fileN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in reader.words(fileP):\n",
    "  print(w + ' ', end='')\n",
    "  if (w is '.'):\n",
    "    print()\n",
    "for w in reader.words(fileN):\n",
    "  print(w + ' ', end='')\n",
    "  if (w is '.'):\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this recipe is to get you to perform a simple counting task on any given corpus. We will be using nltk library's FreqDist object for this purpose here, but more elaboration on the power of FreqDist will follow in the next recipe. Here, we will just concentrate on the application problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['fiction', 'humor', 'romance']\n",
    "whwords = ['what', 'which', 'how', 'why', 'when', 'where', 'who']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FreqDist() accepts a list of words and returns an object that contains the map word and its respective frequency in the input word list. Here, the fdist object will contain the frequency of each of the unique words in the genre_text word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(genres)):\n",
    "    genre = genres[i]\n",
    "    print()\n",
    "    print(\"Analysing '\"+ genre + \"' wh words\")\n",
    "    genre_text = brown.words(categories = genre)\n",
    "    fdist = nltk.FreqDist(genre_text)\n",
    "    for wh in whwords:\n",
    "        print(wh + ':', fdist[wh], end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On analyzing the output, you can clearly see that we have the word count of all seven wh words for the three picked genres on our console. By counting the population of wh words, you can, to a degree, gauge whether the given text is high on relative clauses or question sentences. Similarly, you may have a populated ontology list of important words that you want to get a word count of to understand the relevance of the given text to your ontology. Counting word populations and analyzing distributions of is one of the oldest, simplest, and most popular tricks of the trade to start any kind of textual analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### NLP video from safari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.en import English\n",
    "parser = English()\n",
    "\n",
    "text = \"Mr. A loves tacos. He has Ph.D in Tacology.\"\n",
    "tokens = parser(text)\n",
    "print(tokens)\n",
    "for sent in tokens.sents():\n",
    "    print(\"Sentence:\")\n",
    "    print(sent)\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Nothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', \"Andrew's\", 'text', \"isn't\", 'it']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is Andrew's text isn't it\"\n",
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'Andrew', \"'\", 's', 'text', 'isn', \"'\", 't', 'it']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'Andrew', \"'s\", 'text', 'is', \"n't\", 'it']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"feet cats wolves talked\"\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feet cat wolv talk'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\" \".join(stemmer.stem(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'foot cat wolf talked'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.WordNetLemmatizer()\n",
    "\" \".join(stemmer.lemmatize(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"good move\", \"not a good movie\", \"did not like\", \n",
    "         \"I like it\", \"good one\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2))\n",
    "features = tfidf.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>did</th>\n",
       "      <th>did not</th>\n",
       "      <th>good</th>\n",
       "      <th>good move</th>\n",
       "      <th>good movie</th>\n",
       "      <th>good one</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>like it</th>\n",
       "      <th>move</th>\n",
       "      <th>movie</th>\n",
       "      <th>not</th>\n",
       "      <th>not good</th>\n",
       "      <th>not like</th>\n",
       "      <th>one</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.427993</td>\n",
       "      <td>0.63907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.63907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.330770</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.493899</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.493899</td>\n",
       "      <td>0.398475</td>\n",
       "      <td>0.493899</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.48214</td>\n",
       "      <td>0.48214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.48214</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.614189</td>\n",
       "      <td>0.495524</td>\n",
       "      <td>0.614189</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.427993</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.63907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.63907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       did  did not      good  good move  good movie  good one        it  \\\n",
       "0  0.00000  0.00000  0.427993    0.63907    0.000000   0.00000  0.000000   \n",
       "1  0.00000  0.00000  0.330770    0.00000    0.493899   0.00000  0.000000   \n",
       "2  0.48214  0.48214  0.000000    0.00000    0.000000   0.00000  0.000000   \n",
       "3  0.00000  0.00000  0.000000    0.00000    0.000000   0.00000  0.614189   \n",
       "4  0.00000  0.00000  0.427993    0.00000    0.000000   0.63907  0.000000   \n",
       "\n",
       "       like   like it     move     movie       not  not good  not like  \\\n",
       "0  0.000000  0.000000  0.63907  0.000000  0.000000  0.000000   0.00000   \n",
       "1  0.000000  0.000000  0.00000  0.493899  0.398475  0.493899   0.00000   \n",
       "2  0.388988  0.000000  0.00000  0.000000  0.388988  0.000000   0.48214   \n",
       "3  0.495524  0.614189  0.00000  0.000000  0.000000  0.000000   0.00000   \n",
       "4  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000   0.00000   \n",
       "\n",
       "       one  \n",
       "0  0.00000  \n",
       "1  0.00000  \n",
       "2  0.00000  \n",
       "3  0.00000  \n",
       "4  0.63907  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(features.todense(),\n",
    "             columns = tfidf.get_feature_names()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
